Importing Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from tqdm import tqdm
pd.set_option('display.max_columns',50)
pd.set_option('display.max_rows',50)
plt.style.use('default')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score,f1_score,recall_score,  roc_auc_score,balanced_accuracy_score,jaccard_score,cohen_kappa_score
from sklearn.metrics import matthews_corrcoef

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB, MultinomialNB,ComplementNB

Importing data
odata = pd.read_csv(r"C:\Users\DELL\Downloads\odata2.csv")

Exploratory Data Analysis
odata.head()
odata.tail()
odata.shape
odata['overweight'].value_counts()
odata.isna().sum() #checking for null values
#dropping null values
odata.dropna(inplace = True, axis = 1)
odata.shape
odata.describe()

Splitting
X = odata.drop('overweight', axis =1 )
y = odata['overweight']
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,stratify=y,test_size = 0.2)
print(f'X_train shape : {X_train.shape}\nX_Test shape: {X_test.shape} \ny_train shape : {y_train.shape}\ny_test shape: {y_test.shape}')
y_train.value_counts()

Most of the Naive bayes algorithm do not accept negative values for training, therefore it is not ideal to use a standard scaler which outputs values between -1 to 1. Then best thing to do is to use the min max scaler with range 0 to 1

scaler=MinMaxScaler(feature_range = (0,1))
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

#creating dataframe to store model results
model_results = pd.DataFrame(columns=["Model", "Accuracy Score"])
#Testing various classifiers to see which gives the best accuracy score
models = [
("Gaussian Naive Bayes", GaussianNB()),
("Bernoulli Naive Bayes", BernoulliNB()),
('Multinomial Naive Bayes', MultinomialNB()),
('Complement Naive Bayes', ComplementNB()),
('Categorical Naive Bayes', CategoricalNB())
]

for clf_name, clf in tqdm(models):
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
score = accuracy_score(y_test, predictions)
ypred_prob = clf.predict_proba(X_test)[:, 1]
rocAuc_score = roc_auc_score(y_test, ypred_prob)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
bal_score = balanced_accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
mcc = matthews_corrcoef(y_test, predictions)
cks = cohen_kappa_score(y_test, predictions)
jac = jaccard_score(y_test, predictions)
new_row = {"Model": clf_name, "Accuracy Score": score, 'jaccard score':jac, 'kappa_score':cks,'balanced_acc_score' : bal_score, 'Roc_Auc_score':rocAuc_score,'precision':precision,'recall':recall, 'mcc' : mcc,'f1_score':f1}
model_results = model_results.append(new_row, ignore_index=True)

Results 
model_results.sort_values(by="Accuracy Score", ascending=False)
Defining custom function which returns the list for df.style.apply() method
def highlight_max(s):
if s.dtype == np.object:
is_max = [False for _ in range(s.shape[0])]
else:
is_max = s == s.max()
return ['background: lightgreen' if cell else '' for cell in is_max]

def highlight_min(s):
if s.dtype == np.object:
is_min = [False for _ in range(s.shape[0])]
else:
is_min = s == s.min()
return ['background: red' if cell else '' for cell in is_min]
model_results.style.apply(highlight_max)
model_results.style.apply(highlight_min)


Visualization of results
plt.figure(figsize = (15,4))
sns.barplot(model_results['Model'],model_results['Accuracy Score'],order = model_results.sort_values(by = 'Accuracy Score',ascending = False)['Model'])
plt.title('Barplot of Accuracy Score for models');

plt.figure(figsize = (15,4))
sns.barplot(model_results['Model'],model_results['Roc_Auc_score'], order = model_results.sort_values(by = 'Roc_Auc_score',ascending = False)['Model'])
plt.title('Barplot of Roc_Auc_score for models');

plt.figure(figsize = (15,4))
sns.barplot(model_results['Model'],model_results['precision'], order = model_results.sort_values(by = 'precision',ascending = False)['Model'])
plt.title('Barplot of Precision for models');

plt.figure(figsize = (15,4))
sns.barplot(model_results['Model'],model_results['recall'], order = model_results.sort_values(by = 'recall',ascending = False)['Model'])
plt.title('Barplot of Recall for models');

plt.figure(figsize = (13,6))
plt.plot(model_results['Model'],model_results['jaccard score'],'g--o', label = 'Jaccard score')
plt.plot(model_results['Model'],model_results['precision'],'b--o',label = 'Precision')
plt.plot(model_results['Model'],model_results['recall'],'k--o',label = 'Recall')
plt.legend(loc =[0.44,0.4])
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()

plt.figure(figsize = (13,7))
plt.plot(model_results['Model'],model_results['Accuracy Score'],'g--o', label = 'Accuracy Score')
plt.plot(model_results['Model'],model_results['balanced_acc_score'],'b--o',label = 'balanced_acc_score')
plt.plot(model_results['Model'],model_results['Roc_Auc_score'],'r--o',label = 'Roc_Auc_score')
plt.legend(loc =[1.02,0.85])
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()

plt.figure(figsize = (13,6))
plt.plot(model_results['Model'],model_results['f1_score'],'g--o', label = 'f1_score')
plt.plot(model_results['Model'],model_results['mcc'],'b--o',label = 'mcc')
plt.plot(model_results['Model'],model_results['jaccard score'],'c--o',label = 'jaccard score')
plt.legend(loc =[0.4,0.4])
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()



Handling Data imbalance
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train, y_train)

model_results_resampled = pd.DataFrame(columns=["Model", "Resampled Accuracy Score"])

for clf_name, clf in tqdm(models):
clf.fit(X_res, y_res)
predictions = clf.predict(X_test)
score = accuracy_score(y_test, predictions)
ypred_prob = clf.predict_proba(X_test)[:, 1]
rocAuc_score = roc_auc_score(y_test, ypred_prob)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
bal_score = balanced_accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
mcc = matthews_corrcoef(y_test, predictions)
cks = cohen_kappa_score(y_test, predictions)
jac = jaccard_score(y_test, predictions)
resampled_result = {"Model": clf_name, "Resampled Accuracy Score": score, 'Resampled Jaccard Score':jac, 'Resampled kappa_score':cks,'Resampled Balanced_acc_score' : bal_score, 'Resampled Roc_Auc_score':rocAuc_score,'Resampled precision':precision,'Resampled recall':recall, 'Resampled mcc' : mcc,'Resampled f1_score':f1}

model_results_resampled = model_results_resampled.append(resampled_result,ignore_index=True)
model_results_resampled.sort_values(by="Resampled Accuracy Score", ascending=False)
new_model_results = model_results.set_index('Model').transpose()

plt.figure(figsize = (13,6))
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled Jaccard Score'],'g--o', label = 'Jaccard score')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled precision'],'b--o',label = 'Precision')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled recall'],'k--o',label = 'Recall')
plt.legend(loc =[0.44,0.4])
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()

plt.figure(figsize = (13,6))
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled f1_score'],'g--o', label = 'f1_score')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled mcc'],'b--o',label = 'mcc')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled Jaccard Score'],'c--o',label = 'jaccard score')
plt.legend()
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()

plt.figure(figsize = (13,7))
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled Accuracy Score'],'g--o', label = 'Accuracy Score')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled Balanced_acc_score'],'b--o',label = 'balanced_acc_score')
plt.plot(model_results_resampled['Model'],model_results_resampled['Resampled Roc_Auc_score'],'r--o',label = 'Roc_Auc_score')
plt.legend()
plt.xlabel('Models')
plt.ylabel('Value')
plt.tight_layout()
plt.show()

plt.style.use('ggplot')
ax = new_model_results.plot(kind='bar', figsize=(15, 8), rot=0, colormap = 'rainbow')
ax.legend()
# plt.suptitle("Comparison of Perofrmace metrics for all Naive Bayes Algorithms ", size =15)
plt.savefig('Comparison_of_performance_for_all_Bayes.jpg',bbox_inches='tight')
plt.show()

new_model_results_resampled = model_results_resampled.set_index('Model').transpose()
ax = new_model_results_resampled.plot(kind='bar', figsize=(15, 8), rot=90,colormap = 'rainbow')
ax.legend(loc = [0.6,0.8])
plt.show()

df = pd.merge(model_results,model_results_resampled,on = 'Model').set_index('Model')

ax = df[['mcc','Resampled mcc']].plot(kind='bar', figsize=(15, 6), rot=0)
ax.legend()
plt.show()

models = [("Logistic Regression", LogisticRegression(random_state=101)),

('Categorical Naive Bayes', CategoricalNB()),
("LightGBM", LGBMClassifier(random_state=101)),
("XGBoost", XGBClassifier(random_state=101)),
("CatBoost", CatBoostClassifier(verbose = False,random_state = 101,)),
]
comparison_with_state_of_the_art = pd.DataFrame(columns=["Model", "Accuracy Score"])

for clf_name, clf in tqdm(models):
clf.fit(X_res, y_res)
predictions = clf.predict(X_test)
score = accuracy_score(y_test, predictions)
ypred_prob = clf.predict_proba(X_test)[:, 1]
rocAuc_score = roc_auc_score(y_test, ypred_prob)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
bal_score = balanced_accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
mcc = matthews_corrcoef(y_test, predictions)
cks = cohen_kappa_score(y_test, predictions)
jac = jaccard_score(y_test, predictions)
new_row = {"Model": clf_name, "Accuracy Score": score, 'jaccard score':jac, 'kappa_score':cks,'balanced_acc_score' : bal_score, 'Roc_Auc_score':rocAuc_score,'precision':precision,'recall':recall, 'mcc' : mcc,'f1_score':f1}

comparison_with_state_of_the_art = comparison_with_state_of_the_art.append(new_row, ignore_index=True)

comparison_with_state_of_the_art.sort_values(by="Accuracy Score", ascending=False)
comparison_with_state_of_the_art.style.apply(highlight_min).apply(highlight_max)
comparison_with_state_of_the_art.to_excel('state_of_the_art.xlsx')
comparison_with_state_of_the_art = comparison_with_state_of_the_art.set_index('Model').transpose()

ax = comparison_with_state_of_the_art[['Categorical Naive Bayes','XGBoost']].plot(kind='bar', figsize=(15, 6), rot=0)
ax.legend(loc='upper right')
plt.show()
